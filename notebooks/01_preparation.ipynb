{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094cddd3",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f515e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "path = \"../data_raw/charging_sessions.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff905b",
   "metadata": {},
   "source": [
    "Get an overview over the data through info & head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c47cd9",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938cd80",
   "metadata": {},
   "source": [
    "### Drop the index column \"Unnamed: 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074e5f5",
   "metadata": {},
   "source": [
    "The column has 66450 integer values. However, because of the missing column name it is not clear what this integer value is supposed to represent other than maybe an identifier. We already have the column \"id\" as our identifier, therefore the column \"Unnamed: 0\" does not provide any value to us and we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Drop explicit index column\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65fd65",
   "metadata": {},
   "source": [
    "## Format Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818dca2",
   "metadata": {},
   "source": [
    "### Parse all datetimes in the Localtime America/Los_Angeles for consistency & drop timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_timezones = df[\"timezone\"].dropna().unique()\n",
    "print(\"Unique timezones:\", unique_timezones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c1f0f",
   "metadata": {},
   "source": [
    "Finding: All records share a single timezone (America/Los_Angeles).\n",
    "\n",
    "As all values of this feature are the same and do not provide any value for us by knowing the city, we drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e59037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b330501",
   "metadata": {},
   "source": [
    "The three timestamp columns are therefore parsed as UTC and converted to this local timezone for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = [\"connectionTime\", \"disconnectTime\", \"doneChargingTime\"]\n",
    "local_timezone = unique_timezones[0]\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\")\n",
    "    df[col] = df[col].dt.tz_convert(local_timezone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbf899",
   "metadata": {},
   "source": [
    "### Format time data by adding year, month etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new columns for year, month, day, hour, day of the week and the season\n",
    "df['year'] = df['connectionTime'].dt.year\n",
    "df['month'] = df['connectionTime'].dt.month\n",
    "df['day'] = df['connectionTime'].dt.day\n",
    "df['hour'] = df['connectionTime'].dt.hour\n",
    "df['dayofweek'] = df['connectionTime'].dt.dayofweek\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "    \n",
    "df['season'] = df['month'].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b8fa8",
   "metadata": {},
   "source": [
    "### Parse all values of kwhDelivered in floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont think this is necessary as dytpe is already float\n",
    "print(df['kWhDelivered'].dtype)\n",
    "\n",
    "df['kWhDelivered'] = pd.to_numeric(df['kWhDelivered'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb3d1b",
   "metadata": {},
   "source": [
    "### Use string dytpe and format string for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"siteID\", \"spaceID\", \"stationID\", \"id\", \"sessionID\"]\n",
    "for col in cat_cols:\n",
    "    # print(f'Unique values in {col}: ', df[col].nunique(), f' vs. Unique values in {col} after cleaning: ', df[col].astype(str).str.strip().str.lower().nunique())\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c21177",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d98a4",
   "metadata": {},
   "source": [
    "Finding: The columns doneChargingTime, userID, and userInputs have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda4c41",
   "metadata": {},
   "source": [
    "### Handle missing doneChargingTime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_na = df[\"doneChargingTime\"].isna() & df[\"kWhDelivered\"].gt(0)\n",
    "df_na = df.loc[mask_na].copy()\n",
    "print('COUNT missing doneChargingTime with kWhDelivered > 0: ', mask_na.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184ed91",
   "metadata": {},
   "source": [
    "There are 4,088 records where doneChargingTime is missing. Every one of these records has positive kWhDelivered. This indicates that the charging process occurred normally, but the system failed to log doneChargingTime. These records will be analyzed further to verify that imputing doneChargingTime = disconnectTime is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9691ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na[\"duration_h\"] = (\n",
    "    df_na[\"disconnectTime\"] - df_na[\"connectionTime\"]\n",
    ").dt.total_seconds() / 3600\n",
    "df_na[[\"duration_h\", \"kWhDelivered\"]].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a1292",
   "metadata": {},
   "source": [
    "The missing doneChargingTime sessions show plausible charging behavior:\n",
    "\n",
    "* Average session duration ≈ 4.7 hours, median ≈ 3.9 hours\n",
    "\n",
    "* Average energy delivered ≈ 14 kWh, with max ≈ 78 kWh\n",
    "\n",
    "* No negative or zero durations appear, indicating correct chronological order\n",
    "\n",
    "Although setting doneChargingTime = disconnectTime  represents the latest possible end of charging and may overestimate the true charging duration for some sessions, it is the only defensible imputation given the available data.\n",
    "\n",
    "Dropping these sessions would remove 6.15% of all sessions and introduce systematic bias, while estimating an earlier timestamp would require unavailable information (EV model, SOC, charging curve).\n",
    "\n",
    "Therefore, imputing doneChargingTime = disconnectTime is the most sound approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If doneChargingTime is missing but energy was delivered -> set to disconnectTime\n",
    "df.loc[mask_na, \"doneChargingTime\"] = df.loc[mask_na, \"disconnectTime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e087fb",
   "metadata": {},
   "source": [
    "### Handle missing userID values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4062803",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum()['userID'])\n",
    "print('FRACTION missing userID: ', df['userID'].isna().sum() / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6233b68",
   "metadata": {},
   "source": [
    "As one forth of the userID values are missing it is not plausible to delete these rows. Rather it is better to use placeholder value indicating that no value was given. This placeholder value will be -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({'userID': '-1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea39cf",
   "metadata": {},
   "source": [
    "## Check for invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e0877",
   "metadata": {},
   "source": [
    "### Check for invalid doneChargingTime entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111e356",
   "metadata": {},
   "source": [
    "To ensure temporal consistency, `doneChargingTime` must be between `connectionTime` and `disconnectTime`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = df[\"doneChargingTime\"] < df[\"connectionTime\"]\n",
    "mask_2 = df[\"doneChargingTime\"] > df[\"disconnectTime\"]\n",
    "mask_3 = (df[\"doneChargingTime\"] == df[\"connectionTime\"]) & (df[\"kWhDelivered\"] > 0)\n",
    "print('COUNT doneChargingTime < connectionTime: ', mask_1.sum())\n",
    "print('COUNT doneChargingTime > disconnectTime: ', mask_2.sum())\n",
    "print('COUNT doneChargingTime == connectionTime with energyDelivered > 0: ', mask_3.sum())\n",
    "\n",
    "df_invalid = df[mask_1 | mask_2].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1356d",
   "metadata": {},
   "source": [
    "We identified three types of temporal inconsistencies in the dataset:\n",
    "\n",
    "* `doneChargingTime < connectionTime`: 27 cases  \n",
    "* `doneChargingTime > disconnectTime`: 4,692 cases  \n",
    "* `doneChargingTime == connectionTime with energyDeliverivered > 0`: 2 cases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d5476",
   "metadata": {},
   "source": [
    "We then quantified how far the invalid `doneChargingTime` values deviate from their valid bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many invalid doneChargingTime values are \"close\" (e.g. within 300 seconds)?\n",
    "\n",
    "threshold = 300  # seconds\n",
    "\n",
    "late_off  = (df.loc[mask_2, \"doneChargingTime\"] - df.loc[mask_2, \"disconnectTime\"]).dt.total_seconds().abs()\n",
    "\n",
    "\n",
    "print(f\"LATE   cases: {len(late_off)} sessions\")\n",
    "print(f\"  -> { (late_off <= threshold).mean():.2%} within {threshold} seconds of disconnectTime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1abe8",
   "metadata": {},
   "source": [
    "These results show that the most inconsistencies are extremely small (typically just a few seconds or minutes), indicating minor logging delays rather than invalid sessions. Therefore the most transparent and sound correction is to **clip `doneChargingTime` into the valid interval**. This restores temporal consistency while preserving all meaningful charging sessions for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip doneChargingTime to [connectionTime, disconnectTime]\n",
    "df.loc[mask_2, \"doneChargingTime\"] = df.loc[mask_2, \"disconnectTime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04a64c",
   "metadata": {},
   "source": [
    "For the case `doneChargingTime < connectTime` it is not resonable to clip `doneChargingTime` into the interval & set it at the connectTime. This is due top the fact that the charging duration would then be 0 with positive kWhDelivered, which doesn't make any sense. Because there are just 27 cases we can drop these instances. For the case `doneChargingTime == connectionTime with energyDeliverivered > 0` it is also reasonable dueto the same reasoning to drop these 2 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd726dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_del = mask_1 & mask_3\n",
    "df.drop(df[mask_del].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43d87c",
   "metadata": {},
   "source": [
    "### Check for sessions where the disconnectTime is before connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sessions where disconnectTime < connectionTime\n",
    "print('COUNT disconnectTime < connectionTime: ', (df[\"disconnectTime\"] < df[\"connectionTime\"]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c9181",
   "metadata": {},
   "source": [
    "There are no sessions where disconnectTime < connectionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e4eba",
   "metadata": {},
   "source": [
    "### Check for sessions where kWhDelivered is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sessions where kWhDelivered is negative\n",
    "print('COUNT negative energy rows: ', (df[\"kWhDelivered\"] < 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa4710",
   "metadata": {},
   "source": [
    "There are no sessions where kWhDelivered is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eab1ca",
   "metadata": {},
   "source": [
    "### Check for sessions where duration is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for session with negative duration\n",
    "duration_h = (df[\"disconnectTime\"] - df[\"connectionTime\"]).dt.total_seconds() / 3600.0\n",
    "print('COUNT of sessions <= 0 h: ', (duration_h <= 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b8da6",
   "metadata": {},
   "source": [
    "There are no session where duration is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c168e2",
   "metadata": {},
   "source": [
    "## Handle duplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbdd0f",
   "metadata": {},
   "source": [
    "### Handle duplicates on sessionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db55c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_mask = df[\"sessionID\"].duplicated(keep=False)\n",
    "num_duplicated_sessions = dup_mask.sum()\n",
    "print(\"Rows with duplicated sessionID:\", num_duplicated_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a3177",
   "metadata": {},
   "source": [
    "Each `sessionID` should represent exactly one charging session, but **2,826 rows** appear more than once.    \n",
    "These duplicates indicate repeated logging of the same session and must be consolidated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65da05",
   "metadata": {},
   "source": [
    "We first remove duplicate `sessionID`s (keeping the latest record per session) and then run checks on `id` and the physical key (`stationID`, `connectionTime`) to\n",
    "confirm that no duplicates remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bce688",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cols = [\"sessionID\", \"connectionTime\"]\n",
    "df = df.sort_values(sort_cols)\n",
    "df = df.drop_duplicates(subset=[\"sessionID\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c99640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates on id\n",
    "print(df[\"id\"].duplicated().sum())\n",
    "\n",
    "# Check duplicates on physical key (spaceID, connectionTime)\n",
    "print(df.duplicated(subset=[\"spaceID\", \"connectionTime\"], keep=False).sum())\n",
    "\n",
    "df[df.duplicated(subset=[\"spaceID\", \"connectionTime\"], keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c654a2",
   "metadata": {},
   "source": [
    "## Adding needed Features\n",
    "\n",
    "### Adding Session & Charging duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b42141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute session and charging durations to choose a reporting unit\n",
    "session_duration_minutes = (df[\"disconnectTime\"] - df[\"connectionTime\"]).dt.total_seconds() / 60\n",
    "charging_duration_minutes = (df[\"doneChargingTime\"] - df[\"connectionTime\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Summary stats in minutes and hours\n",
    "stats = pd.DataFrame({\n",
    "    \"mean_minutes\": [session_duration_minutes.mean(), charging_duration_minutes.mean()],\n",
    "    \"median_minutes\": [session_duration_minutes.median(), charging_duration_minutes.median()],\n",
    "    \"mean_hours\": [session_duration_minutes.mean() / 60, charging_duration_minutes.mean() / 60],\n",
    "    \"median_hours\": [session_duration_minutes.median() / 60, charging_duration_minutes.median() / 60],\n",
    "}, index=[\"session_duration\", \"charging_duration\"]).round(2)\n",
    "print(\"Dauer-Statistiken (Minuten/Stunden):\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99b362",
   "metadata": {},
   "source": [
    "We median minutes of both duration is higher than 60. They have a median hour of around 6 & 3 hours witch make it plausible to use hours for the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46041a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['session_duration'] = (df[\"disconnectTime\"] - df[\"connectionTime\"]).dt.total_seconds() / 3600\n",
    "df['charging_duration'] = (df[\"doneChargingTime\"] - df[\"connectionTime\"]).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412db335",
   "metadata": {},
   "source": [
    "### Adding registeredUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isRegisteredUser'] = df['userID'] != '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2324b",
   "metadata": {},
   "source": [
    "## Handle User Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89db186",
   "metadata": {},
   "source": [
    "The last column of the given dataset contains user inputs. We also have to check for missing/errornous values here. Best approach would be to load the user input data into a seperate dataframe containing the sessionID as foreign key.\n",
    "\n",
    "(STILL HAVE TO BE JOINED TOGETHER LATER ON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse user inputs (read the string in the column \"user inputs\" into usable python objects) \n",
    "df_parsed = df.copy()\n",
    "df_parsed[\"userInputs\"] = df_parsed[\"userInputs\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else None)\n",
    "\n",
    "# 1. Create sessions dataframe\n",
    "sessions_df = df_parsed.drop(columns=[\"userInputs\"]).copy()\n",
    "\n",
    "# Create user input dataframe and explode user input column into dictonaries\n",
    "tmp = df_parsed[[\"sessionID\", \"userInputs\"]].copy()\n",
    "tmp = tmp.explode(\"userInputs\")\n",
    "tmp = tmp.dropna(subset=[\"userInputs\"])\n",
    "\n",
    "# Normalize user input dictionaries into columns\n",
    "user_input_df = pd.json_normalize(tmp[\"userInputs\"])\n",
    "user_input_df[\"sessionID\"] = tmp[\"sessionID\"].values\n",
    "\n",
    "user_input_df.info()\n",
    "user_input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c7add",
   "metadata": {},
   "source": [
    "Finding: The resulting dataframe has a total of 9 columns and 61663 rows. There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cd89a",
   "metadata": {},
   "source": [
    "### Convert timestamps to same format as charging_sessions entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93220a",
   "metadata": {},
   "source": [
    "We know that all records share a single timezone (America/Los_Angeles).\n",
    "\n",
    "The two timestamp columns are therefore parsed as UTC and converted to this local timezone for consistency, just as before with the charging sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = [\"modifiedAt\", \"requestedDeparture\"]\n",
    "local_timezone = unique_timezones[0]\n",
    "for col in time_cols:\n",
    "    user_input_df[col] = pd.to_datetime(user_input_df[col], utc=True, errors=\"coerce\")\n",
    "    user_input_df[col] = user_input_df[col].dt.tz_convert(local_timezone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5ba69",
   "metadata": {},
   "source": [
    "## Save processed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007cd4f",
   "metadata": {},
   "source": [
    "First, we have to merge them back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Think about a better way of merging, because the resulting dataset does not contain a suitable id column as primary key\n",
    "# We now have a row for each user input, duplicating the sessionIDs and IDs for multiple user inputs in the same session\n",
    "\n",
    "merged_df = pd.merge(sessions_df, user_input_df, on='sessionID', how='left')\n",
    "merged_df.head()\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864227d9",
   "metadata": {},
   "source": [
    "Finding: We now have multiple userID columns\n",
    "We can check if the IDs align for each entry (setting the value for missing userIDs to \"\" to ensure that only existing values are being compared):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((merged_df[\"userID_x\"].fillna(\"\") == merged_df[\"userID_y\"].fillna(\"\")).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaba9e",
   "metadata": {},
   "source": [
    "This returns true, therefore one of the two user_id columns can be dropped. We drop userID_y and rename userID_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns = ['userID_y'])\n",
    "merged_df.rename(columns={\"userID_x\": \"userID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b275b7",
   "metadata": {},
   "source": [
    "Now we can save that dataframe into a new csv for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b938347",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../data_processed/\"\n",
    "merged_df.to_csv(output_path + \"charging_sessions_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save prepared dataframes as pickle files for use in other notebooks\n",
    "output_path = \"../data_processed/\"\n",
    "\n",
    "with open(output_path + \"df_prepared.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "with open(output_path + \"merged_df_prepared.pkl\", \"wb\") as f:\n",
    "    pickle.dump(merged_df, f)\n",
    "\n",
    "print(\"DataFrames saved as pickle files:\")\n",
    "print(f\"  - {output_path}df_prepared.pkl\")\n",
    "print(f\"  - {output_path}merged_df_prepared.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
